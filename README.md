# LLM-Quality-Observer

[ğŸ‡°ğŸ‡· KR](README.md) | [ğŸ‡ºğŸ‡¸ EN](docs/README-main-us.md)

---


### ê°œìš”

**LLM-Quality-Observer** ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì‘ë‹µ í’ˆì§ˆì„ **ëª¨ë‹ˆí„°ë§í•˜ê³  í‰ê°€**í•˜ê¸° ìœ„í•œ ê°œì¸ MLOps í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.  
ì´ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- LLM ê¸°ë°˜ **Gateway API** êµ¬ì„±
- í”„ë¡¬í”„íŠ¸ / ì‘ë‹µ / ì§€ì—° ì‹œê°„(latency) / ëª¨ë¸ ë²„ì „ ë“±ì„ **DBì— ë¡œê¹…**
- (í–¥í›„) í‰ê°€ ì„œë¹„ìŠ¤(Evaluator)ë¡œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
- (í–¥í›„) ëŒ€ì‹œë³´ë“œì—ì„œ í’ˆì§ˆ/ì§€ì—°/ì—ëŸ¬ìœ¨ ë“± **ì§€í‘œ ì‹œê°í™”**

> í˜„ì¬ ìƒíƒœ: **v1 â€” Gateway API + Postgres ë¡œê¹…ê¹Œì§€ ë™ì‘**

---

### ì•„í‚¤í…ì²˜ ê°œìš”

í˜„ì¬ v1 ì•„í‚¤í…ì²˜:

```mermaid
flowchart TD
    C["Client (Swagger UI / HTTP)"]
    G["Gateway API (FastAPI)"]
    DB["Postgres (table: llm_logs)"]
    E["Evaluator Service (future)"]
    D["Dashboard Service (future)"]

    C --> G
    G -->|LLM call + latency + logging| DB
    DB --> E
    DB --> D
```

### ê¸°ìˆ  ìŠ¤íƒ

* **ì–¸ì–´**: Python 3.12
* **LLM Provider**: OpenAI GPT-5 mini (`responses` API ì‚¬ìš©)
* **ì›¹ í”„ë ˆì„ì›Œí¬**: FastAPI
* **DB**: PostgreSQL 16
* **ORM**: SQLAlchemy
* **ì„¤ì • ê´€ë¦¬**: Pydantic Settings
* **ì˜ì¡´ì„± ê´€ë¦¬**: [`uv`](https://github.com/astral-sh/uv)
* **ì»¨í…Œì´ë„ˆ**: Docker, Docker Compose

---

### í”„ë¡œì íŠ¸ êµ¬ì¡°

ëŒ€ëµì ì¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°:

```text
LLM-Quality-Observer/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ gateway-api/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ db.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â”‚   â””â”€â”€ pyproject.toml
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ evaluator/
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â””â”€â”€ pyproject.toml
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â””â”€â”€ dashboard/
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â””â”€â”€ pyproject.toml
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ docker/
â”‚       â””â”€â”€ docker-compose.local.yml
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ env/
â”‚       â””â”€â”€ .env.local        # local í™˜ê²½ ë³€ìˆ˜ (git ignore ëŒ€ìƒ)
â””â”€â”€ README.md
```

#### `services/gateway-api`

LLM í˜¸ì¶œì„ ë‹´ë‹¹í•˜ëŠ” **Gateway API ì„œë¹„ìŠ¤**ì…ë‹ˆë‹¤.

* `/health` : í—¬ìŠ¤ ì²´í¬
* `/chat` : LLM í˜¸ì¶œ + DB ë¡œê¹…

ì£¼ìš” íŒŒì¼:

* `app/app/main.py`

  * FastAPI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
  * `/health`, `/chat` ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
  * ìµœì´ˆ ì‹¤í–‰ ì‹œ `llm_logs` í…Œì´ë¸” ìƒì„±
  * LLM ì‘ë‹µì„ DBì— ì €ì¥í•˜ê³  `ChatResponse`ë¡œ ë°˜í™˜

* `app/app/config.py`

  * Pydantic `Settings` ì •ì˜
  * í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ:

    * `APP_ENV`
    * `DATABASE_URL`
    * `OPENAI_MODEL_MAIN`
    * `LLM_API_BASE_URL`
    * `LLM_API_KEY`
    * `LOG_LEVEL`

* `app/app/llm_client.py`

  * OpenAI Python SDK ë˜í¼
  * `OPENAI_MODEL_MAIN` ì„ ê¸°ë³¸ ëª¨ë¸ë¡œ ì‚¬ìš©
  * `client.responses.create(...)` í˜¸ì¶œ
  * `(response_text, latency_ms)` íŠœí”Œ ë°˜í™˜

* `app/app/db.py`

  * SQLAlchemy ì—”ì§„ ë° ì„¸ì…˜ ìƒì„±
  * FastAPI `Depends` ë¡œ ì‚¬ìš©í•˜ëŠ” `get_db()` ì œê³µ

* `app/app/models.py`

  * SQLAlchemy ORM ëª¨ë¸: `LLMLog`
  * ì»¬ëŸ¼:

    * `id`, `created_at`
    * `user_id`, `prompt`, `response`
    * `model_version`
    * `latency_ms`
    * `status` (ì˜ˆ: `"success"`)

* `app/app/schemas.py`

  * Pydantic ìŠ¤í‚¤ë§ˆ:

    * `ChatRequest` (ìš”ì²­)
    * `ChatResponse` (ì‘ë‹µ)

* `app/pyproject.toml`

  * gateway-api ì„œë¹„ìŠ¤ìš© Python íŒ¨í‚¤ì§€/ì˜ì¡´ì„± ì •ì˜
  * ë¡œì»¬ ë° Docker ë¹Œë“œ ì‹œ `uv sync`ì— ì‚¬ìš©

#### `services/evaluator` (í–¥í›„ êµ¬í˜„)

* `llm_logs` í…Œì´ë¸”ì„ ì½ì–´ LLM ì‘ë‹µì˜ í’ˆì§ˆ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ì„œë¹„ìŠ¤
* íœ´ë¦¬ìŠ¤í‹±, LLM-as-a-judge, ì‚¬ëŒ í”¼ë“œë°± ë“± ë‹¤ì–‘í•œ ë°©ì‹ì˜ í‰ê°€ë¥¼ ì‹œë„í•  ì˜ˆì •
* í˜„ì¬ëŠ” `pyproject.toml`ê³¼ `Dockerfile`ë§Œ ì¤€ë¹„ëœ ìƒíƒœ (ìŠ¤ì¼ˆë ˆí†¤)

#### `services/dashboard` (í–¥í›„ êµ¬í˜„)

* í’ˆì§ˆ ì§€í‘œ, ì§€ì—° ì‹œê°„, ì—ëŸ¬ìœ¨ ë“±ì„ ì‹œê°í™”í•˜ëŠ” ëŒ€ì‹œë³´ë“œ ì„œë¹„ìŠ¤
* Streamlit ë˜ëŠ” FastAPI ê¸°ë°˜ UIë¥¼ ê³ ë ¤
* ë§ˆì°¬ê°€ì§€ë¡œ `pyproject.toml`ê³¼ `Dockerfile`ë§Œ ì¤€ë¹„ëœ ìƒíƒœ

#### `infra/docker`

* `docker-compose.local.yml`

  * ë¡œì»¬ ê°œë°œìš© Docker Compose ìŠ¤íƒ:

    * `llm-postgres` (Postgres 16)
    * `llm-gateway-api` (FastAPI + OpenAI client)
    * `llm-evaluator` (placeholder)
    * `llm-dashboard` (placeholder)
  * ê¸°ë³¸ì ìœ¼ë¡œ gateway-apië¥¼ `localhost:18000`ì— ë°”ì¸ë”©

#### `configs/env`

* `.env.local`

  * docker-composeì—ì„œ ì°¸ì¡°í•˜ëŠ” local í™˜ê²½ ë³€ìˆ˜ íŒŒì¼
  * ì‹¤ì œ ê²½ë¡œ/íŒŒì¼ëª…ì€ `docker-compose.local.yml` ì˜ `env_file` ì„¤ì •ê³¼ ë§ì¶° ì‚¬ìš©

ì˜ˆì‹œ `.env.local`:

```env
# Application
APP_ENV=local
LOG_LEVEL=DEBUG

# LLM
OPENAI_MODEL_MAIN=gpt-5-mini
LLM_API_BASE_URL=https://api.openai.com/v1
LLM_API_KEY=sk-...

# Database
DATABASE_URL=postgresql://llm_user:llm_password@postgres:5432/llm_quality
```

---

### ë¡œì»¬ ì‹¤í–‰ ë°©ë²• (Docker)

#### 1. ë¦¬í¬ì§€í† ë¦¬ í´ë¡ 

```bash
git clone https://github.com/dongkoony/LLM-Quality-Observer.git
cd LLM-Quality-Observer
```

#### 2. `.env.local` ì„¤ì •

```bash
cp configs/env/.env.local configs/env/.env.local.example  # í•„ìš”ì‹œ ë°±ì—…
# ì´í›„ configs/env/.env.local ë‚´ìš©ì„ ì§ì ‘ ìˆ˜ì •
```

* `LLM_API_KEY` ì— OpenAI API í‚¤ ì…ë ¥
* `OPENAI_MODEL_MAIN` ì„ `gpt-5-mini` ë¡œ ì„¤ì • (ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸)

#### 3. Docker Compose ì‹¤í–‰

```bash
cd infra/docker
docker compose -f docker-compose.local.yml up --build
```

* Gateway API: `http://localhost:18000`
* Postgres: ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ `postgres:5432`

---

### Gateway API ì‚¬ìš©ë²•

#### Health ì²´í¬

```bash
curl http://localhost:18000/health
# -> { "status": "ok" }
```

#### Swagger UI

ë¸Œë¼ìš°ì €ì—ì„œ:

```text
http://localhost:18000/docs
```

ì— ì ‘ì† í›„ `POST /chat` ì—”ë“œí¬ì¸íŠ¸ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥.

#### `/chat` ì˜ˆì‹œ ìš”ì²­

```bash
curl -X POST "http://localhost:18000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain what LLM-Quality-Observer is in one sentence.",
    "user_id": "test-user",
    "model_version": null
  }'
```

ì˜ˆì‹œ ì‘ë‹µ:

```json
{
  "response": "LLM-Quality-Observer is a monitoring and evaluation framework that continuously assesses and tracks the quality of large language model outputs.",
  "model_version": "gpt-5-mini",
  "latency_ms": 4735.19
}
```

ì´ í˜¸ì¶œ ì‹œ:

* OpenAI GPT-5 miniê°€ ì‹¤ì œë¡œ í˜¸ì¶œë˜ê³ 
* ì‘ë‹µê³¼ ì§€ì—° ì‹œê°„ì´ ê³„ì‚°ë˜ë©°
* `llm_logs` í…Œì´ë¸”ì— ë¡œê·¸ê°€ ì €ì¥ë¨

---

### Postgresì—ì„œ ë¡œê·¸ í™•ì¸

```bash
docker exec -it llm-postgres psql -U llm_user -d llm_quality

SELECT id, created_at, user_id,
       LEFT(prompt, 60)   AS prompt_snippet,
       LEFT(response, 60) AS response_snippet,
       model_version,
       latency_ms,
       status
FROM llm_logs
ORDER BY id DESC
LIMIT 10;
```

---

### ë¡œë“œë§µ (Roadmap)

í–¥í›„ ê³„íš:

* **Evaluator Service**

  * `llm_logs` ê¸°ë°˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
  * ê·œì¹™/íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í‰ê°€
  * LLM-as-a-judge í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ í‰ê°€
  * ì‚¬ëŒ í”¼ë“œë°±(RLHF ìŠ¤íƒ€ì¼) ì €ì¥ ë° í™œìš©

* **Dashboard Service**

  * ëª¨ë¸/ë²„ì „ë³„ í‰ê·  ì ìˆ˜
  * ì§€ì—° ì‹œê°„ ë¶„í¬
  * ì—ëŸ¬ìœ¨, ì‹¤íŒ¨ íŒ¨í„´
  * ê¸°ê°„ / ì‚¬ìš©ì / ëª¨ë¸ ë²„ì „ / íƒœê·¸ë³„ í•„í„°ë§

* **Alerting / ì•Œë¦¼**

  * ì ìˆ˜ê°€ íŠ¹ì • ì„ê³„ê°’ ì´í•˜ë¡œ ë–¨ì–´ì§ˆ ë•Œ ì•Œë¦¼
  * p95 latency ê°€ ê¸°ì¤€ì¹˜ë¥¼ ë„˜ì„ ë•Œ ì•Œë¦¼
  * Slack / ì´ë©”ì¼ ì—°ë™

* **Cost Awareness**

  * ëª¨ë¸/ë²„ì „ë³„ í† í° ì‚¬ìš©ëŸ‰ ë° ë¹„ìš© ì¶”ì 
  * í’ˆì§ˆ ì ìˆ˜ì™€ ë¹„ìš©ì„ í•¨ê»˜ ë³´ë©° costâ€“quality íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„

---